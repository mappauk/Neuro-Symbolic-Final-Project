{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhV44Kp3nh2t"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "!pip install tensorflow-text\n",
        "!pip install tf-models-official\n",
        "!pip install tensorflow-addons\n",
        "!pip install scikit-learn\n",
        "!pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaP5lSz-n1cu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from official.nlp import optimization\n",
        "import tensorflow_addons as tfa\n",
        "import transformers\n",
        "import sklearn as sk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yW68-JwDoNOf"
      },
      "outputs": [],
      "source": [
        "node_list_task_two = {\n",
        "    \"Logos\": 0,\n",
        "    \"Repetition\": 1,\n",
        "    \"Obfuscation, Intentional vagueness, Confusion\": 2,\n",
        "    \"Reasoning\": 3,\n",
        "    \"Justification\": 4,\n",
        "    \"Slogans\": 5,\n",
        "    \"Bandwagon\": 6,\n",
        "    \"Appeal to authority\": 7,\n",
        "    \"Flag-waving\": 8,\n",
        "    \"Appeal to fear/prejudice\": 9,\n",
        "    \"Simplification\": 10,\n",
        "    \"Causal Oversimplification\": 11,\n",
        "    \"Black-and-white Fallacy/Dictatorship\": 12,\n",
        "    \"Thought-terminating cliché\": 13,\n",
        "    \"Distraction\": 14,\n",
        "    \"Misrepresentation of Someone's Position (Straw Man)\": 15,\n",
        "    \"Presenting Irrelevant Data (Red Herring)\": 16,\n",
        "    \"Whataboutism\": 17,\n",
        "    \"Ethos\": 18,\n",
        "    \"Glittering generalities (Virtue)\": 19,\n",
        "    \"Ad Hominem\": 20,\n",
        "    \"Doubt\": 21,\n",
        "    \"Name calling/Labeling\": 22,\n",
        "    \"Smears\": 23,\n",
        "    \"Reductio ad hitlerum\": 24,\n",
        "    \"Pathos\": 25,\n",
        "    \"Exaggeration/Minimisation\": 26,\n",
        "    \"Loaded Language\": 27,\n",
        "    \"Transfer\": 28,\n",
        "    \"Appeal to (Strong) Emotions\": 29\n",
        "}\n",
        "\n",
        "id_to_node = {\n",
        "    0: \"Logos\",\n",
        "    1: \"Repetition\",\n",
        "    2: \"Obfuscation, Intentional vagueness, Confusion\",\n",
        "    3: \"Reasoning\",\n",
        "    4: \"Justification\",\n",
        "    5: \"Slogans\",\n",
        "    6: \"Bandwagon\",\n",
        "    7: \"Appeal to authority\",\n",
        "    8: \"Flag-waving\",\n",
        "    9: \"Appeal to fear/prejudice\",\n",
        "    10: \"Simplification\",\n",
        "    11: \"Causal Oversimplification\",\n",
        "    12: \"Black-and-white Fallacy/Dictatorship\",\n",
        "    13: \"Thought-terminating cliché\",\n",
        "    14: \"Distraction\",\n",
        "    15: \"Misrepresentation of Someone's Position (Straw Man)\",\n",
        "    16: \"Presenting Irrelevant Data (Red Herring)\",\n",
        "    17: \"Whataboutism\",\n",
        "    18: \"Ethos\",\n",
        "    19: \"Glittering generalities (Virtue)\",\n",
        "    20: \"Ad Hominem\",\n",
        "    21: \"Doubt\",\n",
        "    22: \"Name calling/Labeling\",\n",
        "    23: \"Smears\",\n",
        "    24: \"Reductio ad hitlerum\",\n",
        "    25: \"Pathos\",\n",
        "    26: \"Exaggeration/Minimisation\",\n",
        "    27: \"Loaded Language\",\n",
        "    28: \"Transfer\",\n",
        "    29: \"Appeal to (Strong) Emotions\"\n",
        "}\n",
        "\n",
        "parent_child_dict = {\n",
        "    \"Logos\": [],\n",
        "    \"Repetition\": [\"Logos\"],\n",
        "    \"Obfuscation, Intentional vagueness, Confusion\": [\"Logos\"],\n",
        "    \"Reasoning\": [\"Logos\"],\n",
        "    \"Justification\": [\"Logos\"],\n",
        "    \"Slogans\": [\"Justification\", \"Logos\"],\n",
        "    \"Bandwagon\": [\"Justification\", \"Ethos\", \"Logos\"],\n",
        "    \"Appeal to authority\": [\"Justification\", \"Ethos\", \"Logos\"],\n",
        "    \"Flag-waving\": [\"Justification\", \"Pathos\", \"Logos\"],\n",
        "    \"Appeal to fear/prejudice\": [\"Justification\", \"Pathos\", \"Logos\"],\n",
        "    \"Simplification\": [\"Reasoning\", \"Logos\"],\n",
        "    \"Causal Oversimplification\": [\"Simplification\", \"Reasoning\", \"Logos\"],\n",
        "    \"Black-and-white Fallacy/Dictatorship\": [\"Simplification\", \"Reasoning\", \"Logos\"],\n",
        "    \"Thought-terminating cliché\": [\"Simplification\", \"Reasoning\", \"Logos\"],\n",
        "    \"Distraction\": [\"Reasoning\", \"Logos\"],\n",
        "    \"Misrepresentation of Someone's Position (Straw Man)\": [\"Distraction\", \"Reasoning\", \"Logos\"],\n",
        "    \"Presenting Irrelevant Data (Red Herring)\": [\"Distraction\", \"Reasoning\", \"Logos\"],\n",
        "    \"Whataboutism\": [\"Distraction\", \"Reasoning\", \"Logos\"],\n",
        "    \"Ethos\": [],\n",
        "    \"Glittering generalities (Virtue)\": [\"Ethos\"],\n",
        "    \"Ad Hominem\": [\"Ethos\"],\n",
        "    \"Doubt\": [\"Ad Hominem\", \"Ethos\"],\n",
        "    \"Name calling/Labeling\": [\"Ad Hominem\", \"Ethos\"],\n",
        "    \"Smears\": [\"Ad Hominem\", \"Ethos\"],\n",
        "    \"Reductio ad hitlerum\": [\"Ad Hominem\", \"Ethos\"],\n",
        "    \"Pathos\": [],\n",
        "    \"Exaggeration/Minimisation\": [\"Pathos\"],\n",
        "    \"Loaded Language\": [\"Pathos\"],\n",
        "    \"Transfer\": [\"Ethos\", \"Pathos\"],\n",
        "    \"Appeal to (Strong) Emotions\": [\"Pathos\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2DzY45OokTY"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "input_directory = '/content/drive/MyDrive/2023-2024 School Year/Fall Semester/Neuro-Symbolic Approaches to NLP/ProjectData'\n",
        "subtask_2_train = input_directory + '/subtask2a/train.json'\n",
        "subtask_2_validation = input_directory + '/subtask2a/validation.json'\n",
        "base_train_image_path = input_directory + '/train_images/'\n",
        "base_dev_image_path = input_directory + '/validation_images/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Y41LLpNoo-9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def extract_data(filename, base_image_path):\n",
        "  id_dict = {}\n",
        "  ids = []\n",
        "  text = []\n",
        "  labels = []\n",
        "  images = []\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    for elem in data:\n",
        "      id_dict[elem[\"id\"]] = len(ids)\n",
        "      ids.append(elem[\"id\"])\n",
        "      text.append(elem[\"text\"])\n",
        "      labels.append(elem[\"labels\"])\n",
        "      images.append(tf.keras.utils.load_img(base_image_path + elem[\"image\"]).resize((224,224)))\n",
        "  return id_dict, ids, text, images, labels\n",
        "\n",
        "def get_leaf_encoded_labels(plaintext_labels_list, label_map):\n",
        "  labels = []\n",
        "  for plaintext_labels in plaintext_labels_list:\n",
        "    example_labels = np.zeros(len(label_map))\n",
        "    for plaintext_label in plaintext_labels:\n",
        "      example_labels[label_map[plaintext_label]] = 1\n",
        "    labels.append(example_labels)\n",
        "  return labels\n",
        "\n",
        "def get_hierarchy_encoded_labels(plaintext_labels_list, label_map, child_parent_map):\n",
        "  labels = []\n",
        "  for plaintext_labels in plaintext_labels_list:\n",
        "    example_labels = np.zeros(len(label_map))\n",
        "    for plaintext_label in plaintext_labels:\n",
        "      example_labels[label_map[plaintext_label]] = 1\n",
        "      for ancestor_plaintext in child_parent_map[plaintext_label]:\n",
        "        example_labels[label_map[ancestor_plaintext]] = 1\n",
        "    labels.append(example_labels)\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "plQLi8a_oq9F"
      },
      "outputs": [],
      "source": [
        "train_id_dict, raw_train_ids, raw_train_text, raw_train_images, raw_train_labels = extract_data(subtask_2_train, base_train_image_path)\n",
        "dev_id_dict, raw_dev_ids, raw_dev_text, raw_dev_images, raw_dev_labels = extract_data(subtask_2_validation, base_dev_image_path)\n",
        "\n",
        "encoded_train_labels = np.array(get_leaf_encoded_labels(raw_train_labels, node_list_task_two))\n",
        "encoded_hierarchy_train_labels = np.array(get_hierarchy_encoded_labels(raw_train_labels, node_list_task_two, parent_child_dict))\n",
        "encoded_dev_labels = np.array(get_leaf_encoded_labels(raw_dev_labels, node_list_task_two))\n",
        "encoded_hierarchy_dev_labels = np.array(get_hierarchy_encoded_labels(raw_dev_labels, node_list_task_two, parent_child_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EulueggEN73Q"
      },
      "outputs": [],
      "source": [
        "encoded_train_labels = np.array(encoded_train_labels)\n",
        "encoded_hierarchy_train_labels = np.array(encoded_hierarchy_train_labels)\n",
        "encoded_dev_labels = np.array(encoded_dev_labels)\n",
        "encoded_hierarchy_dev_labels = np.array(encoded_hierarchy_dev_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAeKO-qUvGyk"
      },
      "outputs": [],
      "source": [
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "# If using train set for evaluation\n",
        "final_train_ids, final_train_labels, final_eval_ids, final_eval_labels = iterative_train_test_split(np.expand_dims(np.array(raw_train_ids), axis=1), encoded_hierarchy_train_labels, test_size=0.07)\n",
        "final_train_ids = final_train_ids.squeeze()\n",
        "final_eval_ids = final_eval_ids.squeeze()\n",
        "\n",
        "final_raw_train_text = []\n",
        "final_raw_train_images = []\n",
        "for train_id in final_train_ids:\n",
        "  final_raw_train_text.append(raw_train_text[train_id_dict[train_id]])\n",
        "  final_raw_train_images.append(raw_train_images[train_id_dict[train_id]])\n",
        "\n",
        "final_raw_eval_text = []\n",
        "final_raw_eval_images = []\n",
        "for train_id in final_eval_ids:\n",
        "  final_raw_eval_text.append(raw_train_text[train_id_dict[train_id]])\n",
        "  final_raw_eval_images.append(raw_train_images[train_id_dict[train_id]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LP2LjnBSpDN8"
      },
      "outputs": [],
      "source": [
        "# If using dev set for evaluation\n",
        "final_raw_train_text, final_raw_train_images, final_train_labels = sk.utils.shuffle(raw_train_text, raw_train_images, encoded_hierarchy_train_labels)\n",
        "final_raw_eval_text = raw_dev_text\n",
        "final_raw_eval_images = raw_dev_images\n",
        "final_eval_labels = encoded_hierarchy_dev_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gpX6AptvwoG"
      },
      "outputs": [],
      "source": [
        "# only run on google runtime\n",
        "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "bert_encoder = transformers.TFDebertaModel.from_pretrained(\"microsoft/deberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHTICB5VxajA"
      },
      "outputs": [],
      "source": [
        "# CNN Based\n",
        "image_processor = transformers.AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
        "image_model = transformers.TFResNetModel.from_pretrained(\"microsoft/resnet-50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XgnTWto9wERr"
      },
      "outputs": [],
      "source": [
        "def hugging_face_bert_encode(text_data):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  i = 0\n",
        "  for text in text_data:\n",
        "    i += 1\n",
        "    tokenized_data = bert_tokenizer(text, padding='max_length', max_length=512, truncation=True)\n",
        "    input_ids.append(tokenized_data['input_ids'])\n",
        "    attention_masks.append(tokenized_data['attention_mask'])\n",
        "  return [np.array(input_ids), np.array(attention_masks)]\n",
        "\n",
        "def image_encode(image_data):\n",
        "  pixel_values = []\n",
        "  for image in image_data:\n",
        "    pixel_values.append(image_processor(image)[\"pixel_values\"])\n",
        "  return np.array(pixel_values)\n",
        "\n",
        "hugging_face_train_text_data = hugging_face_bert_encode(final_raw_train_text)\n",
        "hugging_face_train_image_data = image_encode(final_raw_train_images).squeeze()\n",
        "\n",
        "hugging_face_eval_text_data = hugging_face_bert_encode(final_raw_eval_text)\n",
        "hugging_face_eval_image_data = image_encode(final_raw_eval_images).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YDESj-ZSDEuy"
      },
      "outputs": [],
      "source": [
        "final_train_data = {}\n",
        "final_train_data[\"input_ids\"] = hugging_face_train_text_data[0]\n",
        "final_train_data[\"attention_mask\"] = hugging_face_train_text_data[1]\n",
        "final_train_data[\"images\"] = hugging_face_train_image_data\n",
        "final_train_labels = final_train_labels\n",
        "final_eval_data = {}\n",
        "final_eval_data[\"input_ids\"] = hugging_face_eval_text_data[0]\n",
        "final_eval_data[\"attention_mask\"] = hugging_face_eval_text_data[1]\n",
        "final_eval_data[\"images\"] = hugging_face_eval_image_data\n",
        "final_eval_labels = final_eval_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WvS9n-dJDIdf"
      },
      "outputs": [],
      "source": [
        "class Hierarchy_Rule:\n",
        "  def __init__(self, num_classes, parent_index, child_index):\n",
        "     # p_student is 2D array of size n x k, where n is number of input examples and k is number of classes\n",
        "     self.num_classes = num_classes\n",
        "     self.parent_encoding = tf.squeeze(tf.one_hot([parent_index], self.num_classes))\n",
        "     self.child_encoding = tf.squeeze(tf.one_hot([child_index], self.num_classes))\n",
        "     self.parent_index = parent_index\n",
        "     self.child_index = child_index\n",
        "\n",
        "  @tf.function\n",
        "  def rule_evaluation(self, p_student):\n",
        "    return tf.math.minimum((1 - tf.tensordot(p_student, self.child_encoding, 1)) + tf.tensordot(p_student, self.parent_encoding, 1), 1)\n",
        "\n",
        "  @tf.function\n",
        "  def log_distribution(self, p_student, regularization_term, confidence_val):\n",
        "    log_dist = tf.ones_like(p_student)\n",
        "    rule_perf = tf.ones_like(p_student)\n",
        "    rule_eval = self.rule_evaluation(p_student)\n",
        "    child_indicies = tf.stack([tf.range(tf.shape(log_dist)[0]), tf.fill([tf.shape(log_dist)[0]], self.child_index)], axis=1)\n",
        "    log_dist = tf.tensor_scatter_nd_update(log_dist, child_indicies, rule_eval)\n",
        "    test_val = float(-1)*float(regularization_term)*float(confidence_val)\n",
        "    sub_test_val = (rule_perf - log_dist)\n",
        "    output = test_val*sub_test_val\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hY7jZrL5DS0F"
      },
      "outputs": [],
      "source": [
        "class TeacherNetwork(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(TeacherNetwork, self).__init__()\n",
        "    self.rules = [\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Logos\"], node_list_task_two[\"Repetition\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Logos\"], node_list_task_two[\"Obfuscation, Intentional vagueness, Confusion\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Logos\"], node_list_task_two[\"Reasoning\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Logos\"], node_list_task_two[\"Justification\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Justification\"], node_list_task_two[\"Slogans\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Justification\"], node_list_task_two[\"Bandwagon\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Justification\"], node_list_task_two[\"Appeal to authority\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Justification\"], node_list_task_two[\"Flag-waving\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Justification\"], node_list_task_two[\"Appeal to fear/prejudice\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Reasoning\"], node_list_task_two[\"Repetition\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Logos\"], node_list_task_two[\"Simplification\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Simplification\"], node_list_task_two[\"Causal Oversimplification\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Simplification\"], node_list_task_two[\"Black-and-white Fallacy/Dictatorship\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Simplification\"], node_list_task_two[\"Thought-terminating cliché\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Reasoning\"], node_list_task_two[\"Distraction\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Distraction\"], node_list_task_two[\"Misrepresentation of Someone's Position (Straw Man)\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Distraction\"], node_list_task_two[\"Presenting Irrelevant Data (Red Herring)\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Distraction\"], node_list_task_two[\"Whataboutism\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ethos\"], node_list_task_two[\"Appeal to authority\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ethos\"], node_list_task_two[\"Glittering generalities (Virtue)\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ethos\"], node_list_task_two[\"Bandwagon\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ethos\"], node_list_task_two[\"Ad Hominem\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ethos\"], node_list_task_two[\"Transfer\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ad Hominem\"], node_list_task_two[\"Doubt\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ad Hominem\"], node_list_task_two[\"Name calling/Labeling\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ad Hominem\"], node_list_task_two[\"Smears\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ad Hominem\"], node_list_task_two[\"Reductio ad hitlerum\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Ad Hominem\"], node_list_task_two[\"Whataboutism\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Exaggeration/Minimisation\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Loaded Language\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Appeal to (Strong) Emotions\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Appeal to fear/prejudice\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Flag-waving\"]),\n",
        "      Hierarchy_Rule(len(node_list_task_two), node_list_task_two[\"Pathos\"], node_list_task_two[\"Transfer\"]),\n",
        "    ]\n",
        "    self.rule_lambdas = tf.fill([len(self.rules)], 100)\n",
        "    self.regularization_term = 1\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.batch_size = input_shape\n",
        "    return\n",
        "\n",
        "  def call(self, inputs):\n",
        "    student_probs = inputs[0]\n",
        "    rule_distr = self.calculate_rule_constraints(student_probs, self.rules, self.rule_lambdas, self.regularization_term)\n",
        "    rule_adj_probs = tf.math.multiply(student_probs, rule_distr)\n",
        "    return rule_adj_probs\n",
        "\n",
        "  def calculate_rule_constraints(self, input, rules, rule_confidences, C):\n",
        "    distr_total = tf.zeros_like(input)\n",
        "    for i in range(len(rules)):\n",
        "      distr = rules[i].log_distribution(input, C, rule_confidences[i])\n",
        "      distr_total = distr_total + distr\n",
        "    distr_total = tf.clip_by_value(distr_total, -60, 60)\n",
        "    return tf.math.exp(distr_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4yP1rCFtDYrH"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "  # student network\n",
        "  text_input_length = 512\n",
        "  label_count = 30\n",
        "  input_ids = tf.keras.Input(shape=(text_input_length,),dtype='int32')\n",
        "  attention_masks = tf.keras.Input(shape=(text_input_length,),dtype='int32')\n",
        "  pixel_inputs = tf.keras.Input(shape=(3, 224, 224), dtype='float32')\n",
        "  bert_outputs = bert_encoder([input_ids, attention_masks])['last_hidden_state']\n",
        "  vision_outputs = image_model(pixel_inputs)['pooler_output']\n",
        "  vision_pooling_layer = tf.keras.layers.GlobalMaxPool2D(data_format='channels_first', keepdims=False)\n",
        "  bert_pooling_layer = tf.keras.layers.GlobalMaxPool1D()\n",
        "  bert_pooled = bert_pooling_layer(bert_outputs)\n",
        "  vision_pooled = vision_pooling_layer(vision_outputs)\n",
        "  combined_bert_vision = tf.keras.layers.Concatenate()([bert_pooled, vision_pooled])\n",
        "  dropout = tf.keras.layers.Dropout(0.1)\n",
        "  hidden_dense_inputs = dropout(combined_bert_vision)\n",
        "  hidden_dense = tf.keras.layers.Dense(int(combined_bert_vision.shape[1]/2), activation='relu')\n",
        "  classifier_inputs = hidden_dense(hidden_dense_inputs)\n",
        "  classifier = tf.keras.layers.Dense(label_count, activation='sigmoid', name='output')\n",
        "  outputs = classifier(classifier_inputs)\n",
        "  # teacher network\n",
        "  teacher_network = TeacherNetwork()\n",
        "  teacher_outputs = teacher_network([outputs])\n",
        "  concatenate_outputs = tf.keras.layers.Concatenate(axis=0, trainable=False)\n",
        "  return tf.keras.Model([input_ids, attention_masks, pixel_inputs], concatenate_outputs([outputs, teacher_outputs]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5YJmj4-TDd1f"
      },
      "outputs": [],
      "source": [
        "# hierarchial f1 implementation is based on provided scorer by semeval task organizers, found by registering here: https://propaganda.math.unipd.it/semeval2024task4/\n",
        "# their f1 implementation is in turn based on sklearn implementation found here: https://github.com/globality-corp/sklearn-hierarchical-classification/blob/1de19f782d992a82dace895f9c24a0fc074baeeb/sklearn_hierarchical_classification/metrics.py#L201\n",
        "threshold = 0.5\n",
        "\n",
        "@tf.function\n",
        "def macro_f1_helper(y_true, y_pred):\n",
        "  # small nonzero to avoid any divide by zero issues\n",
        "  small_nonzero = 0.00000000001\n",
        "  true_positives = tf.math.count_nonzero(y_true * y_pred, axis=0, dtype=tf.float64)\n",
        "  all_positives = tf.math.count_nonzero(y_true, axis=0, dtype=tf.float64)\n",
        "  predicted_positives = tf.math.count_nonzero(y_pred, axis=0, dtype=tf.float64)\n",
        "  precision = true_positives / (predicted_positives + small_nonzero)\n",
        "  recall = true_positives / (all_positives + small_nonzero)\n",
        "  f1 = 2 * precision * recall / (precision + recall + small_nonzero)\n",
        "  return tf.math.reduce_mean(f1)\n",
        "\n",
        "@tf.function\n",
        "def h_recall_score(y_true, y_pred):\n",
        "    true_positives = tf.math.count_nonzero(y_true * y_pred, dtype=tf.float64)\n",
        "    all_positives = tf.math.count_nonzero(y_true, dtype=tf.float64)\n",
        "    return true_positives / all_positives\n",
        "\n",
        "@tf.function\n",
        "def h_precision_score(y_true, y_pred):\n",
        "    true_positives = tf.math.count_nonzero(y_true * y_pred, dtype=tf.float64)\n",
        "    all_results = tf.math.count_nonzero(y_pred, dtype=tf.float64)\n",
        "    return true_positives / all_results\n",
        "\n",
        "@tf.function\n",
        "def hierarchial_f1_helper(y_true, y_pred, beta=1.):\n",
        "    hP = h_precision_score(y_true, y_pred)\n",
        "    hR = h_recall_score(y_true, y_pred)\n",
        "    hF = (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
        "    return hP, hR, hF\n",
        "\n",
        "# Metric Functions\n",
        "@tf.function\n",
        "def macro_f1_student(y_true, y_pred):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_student = y_pred[0:output_divide,:]\n",
        "    p_student_rounded = tf.where(p_student > threshold, 1, 0)\n",
        "    return macro_f1_helper(tf.cast(y_true, tf.int32), tf.cast(p_student_rounded, tf.int32))\n",
        "\n",
        "@tf.function\n",
        "def macro_f1_teacher(y_true, y_pred):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_teacher = y_pred[output_divide: y_pred.shape[0], :]\n",
        "    p_teacher_rounded = tf.where(p_teacher > threshold, 1, 0)\n",
        "    return macro_f1_helper(tf.cast(y_true, tf.int32), tf.cast(p_teacher_rounded, tf.int32))\n",
        "\n",
        "@tf.function\n",
        "def hierarchial_f1_student(y_true, y_pred):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_student = y_pred[0:output_divide,:]\n",
        "    p_student_rounded = tf.where(p_student > threshold, 1, 0)\n",
        "    return hierarchial_f1_helper(tf.cast(y_true, tf.int32), tf.cast(p_student_rounded, tf.int32))\n",
        "\n",
        "@tf.function\n",
        "def hierarchial_f1_teacher(y_true, y_pred):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_teacher = y_pred[output_divide: y_pred.shape[0], :]\n",
        "    p_teacher_rounded = tf.where(p_teacher > threshold, 1, 0)\n",
        "    return hierarchial_f1_helper(tf.cast(y_true, tf.int32), tf.cast(p_teacher_rounded, tf.int32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tb3jt_pquaIo"
      },
      "outputs": [],
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "checkpoint_save_dir = \"/tmp/model\"\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y, model, loss, step):\n",
        "  with tf.GradientTape() as tape:\n",
        "    probs = model(x, training=True)\n",
        "    student_loss, teacher_loss, final_loss = loss(y, probs, step)\n",
        "  grads = tape.gradient(final_loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "  return probs, y, student_loss, teacher_loss, final_loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y, model, loss, step):\n",
        "  probs = model(x, training=False)\n",
        "  student_loss, teacher_loss, final_loss = loss(y, probs, step)\n",
        "  return probs, y, student_loss, teacher_loss, final_loss\n",
        "\n",
        "def print_metrics(expected_vals, total_preds):\n",
        "  print(f\"macro f1 teacher: {macro_f1_teacher(expected_vals, total_preds)}\")\n",
        "  print(f\"macro f1 student: {macro_f1_student(expected_vals, total_preds)}\")\n",
        "  student_hprec, student_hrec, student_hf1 =  hierarchial_f1_student(expected_vals, total_preds)\n",
        "  teacher_hprec, teacher_hrec, teacher_hf1 =  hierarchial_f1_teacher(expected_vals, total_preds)\n",
        "  print(f\"Student Hierarchial; precision: {student_hprec}, recall: {student_hrec}, f1: {student_hf1}\")\n",
        "  print(f\"Teacher Hierarchial; precision: {teacher_hprec}, recall: {teacher_hrec}, f1: {teacher_hf1}\")\n",
        "\n",
        "def training_loop(\n",
        "  model,\n",
        "  train_input_ids,\n",
        "  train_attention_masks,\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  dev_input_ids,\n",
        "  dev_attention_masks,\n",
        "  dev_images,\n",
        "  dev_labels,\n",
        "  epochs,\n",
        "  batch_size,\n",
        "  optimizer,\n",
        "  loss,\n",
        "  checkpoint\n",
        "):\n",
        "  steps_per_epoch = int(train_input_ids.shape[0] / batch_size)\n",
        "  dev_steps_per_epoch = int(dev_input_ids.shape[0] / batch_size)\n",
        "  num_train_steps = int(steps_per_epoch * epochs)\n",
        "  current_step = 0\n",
        "  manager = tf.train.CheckpointManager(checkpoint, directory=\"/tmp/model\", max_to_keep=epochs)\n",
        "  for epoch in range(epochs):\n",
        "      print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "      # Iterate over the batches of the dataset.\n",
        "      train_teacher_loss_list = []\n",
        "      train_student_loss_list = []\n",
        "      train_final_loss_list = []\n",
        "      train_preds_student_list = []\n",
        "      train_preds_teacher_list = []\n",
        "      train_expected_list = []\n",
        "      for step in tqdm(range(steps_per_epoch)):\n",
        "        # get current batch and run through model\n",
        "        current_step += 1\n",
        "        batch_input_ids = train_input_ids[step*batch_size:(step + 1)*batch_size, :]\n",
        "        batch_attention_masks = train_attention_masks[step*batch_size:(step + 1)*batch_size, :]\n",
        "        batch_images = train_images[step*batch_size:(step + 1)*batch_size, :, :, :]\n",
        "        batch_labels = train_labels[step*batch_size:(step + 1)*batch_size, :]\n",
        "        preds, expected, student_loss, teacher_loss, final_loss = train_step([batch_input_ids, batch_attention_masks, batch_images], batch_labels, model, loss, tf.convert_to_tensor(current_step, tf.float32))\n",
        "        # update predictions for metric evaluation at the end\n",
        "        output_divide = int(preds.shape[0]/2)\n",
        "        p_student = preds[0:output_divide, :]\n",
        "        p_teacher = preds[output_divide: preds.shape[0], :]\n",
        "        train_preds_student_list.append(p_student)\n",
        "        train_preds_teacher_list.append(p_teacher)\n",
        "        train_student_loss_list.append(student_loss)\n",
        "        train_teacher_loss_list.append(teacher_loss)\n",
        "        train_final_loss_list.append(final_loss)\n",
        "        train_expected_list.append(expected)\n",
        "      # aggregate all predictions and print loss + metrics for training set\n",
        "      train_preds_teacher = tf.concat(train_preds_teacher_list, 0)\n",
        "      train_preds_student = tf.concat(train_preds_student_list, 0)\n",
        "      total_preds = tf.concat([train_preds_student, train_preds_teacher], 0)\n",
        "      expected_vals = tf.concat(train_expected_list, 0)\n",
        "      print(\"Training Data Results:\")\n",
        "      print(f\"student loss: {sum(train_student_loss_list)/len(train_student_loss_list)}\")\n",
        "      print(f\"teacher loss: {sum(train_teacher_loss_list)/len(train_teacher_loss_list)}\")\n",
        "      print(f\"final loss: {sum(train_final_loss_list)/len(train_final_loss_list)}\")\n",
        "      print_metrics(expected_vals, total_preds)\n",
        "      manager.save(checkpoint_number=epoch)\n",
        "\n",
        "      test_teacher_loss_list = []\n",
        "      test_student_loss_list = []\n",
        "      test_final_loss_list = []\n",
        "      test_preds_student_list = []\n",
        "      test_preds_teacher_list = []\n",
        "      test_expected_list = []\n",
        "      # Run a validation loop at the end of each epoch.\n",
        "      for i in range(dev_steps_per_epoch):\n",
        "        # get current batch and run through model\n",
        "        batch_input_ids = dev_input_ids[i*batch_size:(i + 1)*batch_size, :]\n",
        "        batch_attention_masks = dev_attention_masks[i*batch_size:(i + 1)*batch_size, :]\n",
        "        batch_labels = dev_labels[i*batch_size:(i + 1)*batch_size, :]\n",
        "        batch_images = dev_images[i*batch_size:(i + 1)*batch_size, :, :, :]\n",
        "        batch_images = tf.convert_to_tensor(batch_images)\n",
        "        preds, expected, student_loss, teacher_loss, final_loss = test_step([batch_input_ids, batch_attention_masks, batch_images], batch_labels, model, loss, tf.convert_to_tensor(current_step, tf.float32))\n",
        "        output_divide = int(preds.shape[0]/2)\n",
        "        # update predictions for metric evaluation at the end\n",
        "        p_student = preds[0:output_divide, :]\n",
        "        p_teacher = preds[output_divide: preds.shape[0], :]\n",
        "        test_preds_student_list.append(p_student)\n",
        "        test_preds_teacher_list.append(p_teacher)\n",
        "        test_student_loss_list.append(student_loss)\n",
        "        test_teacher_loss_list.append(teacher_loss)\n",
        "        test_final_loss_list.append(final_loss)\n",
        "        test_expected_list.append(expected)\n",
        "      # aggregate all predictions and print loss + metrics for validation set\n",
        "      test_student_preds = tf.concat(test_preds_student_list, 0)\n",
        "      test_teacher_preds = tf.concat(test_preds_teacher_list, 0)\n",
        "      test_preds = tf.concat([test_student_preds, test_teacher_preds], 0)\n",
        "      test_expected_vals = tf.concat(test_expected_list, 0)\n",
        "      print(\"\\nTest Data Results:\")\n",
        "      print(f\"student loss: {sum(test_student_loss_list)/len(test_student_loss_list)}\")\n",
        "      print(f\"teacher loss: {sum(test_teacher_loss_list)/len(test_teacher_loss_list)}\")\n",
        "      print(f\"final loss: {sum(test_final_loss_list)/len(test_final_loss_list)}\")\n",
        "      print_metrics(test_expected_vals, test_preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yIOSEh77u9cO"
      },
      "outputs": [],
      "source": [
        "# loss function for equal weighted teacher and student losses\n",
        "def custom_loss_wrapper():\n",
        "  bce_loss= tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "  kl_loss = tf.keras.losses.KLDivergence()\n",
        "  def custom_loss(y_true, y_pred, step):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_student = y_pred[0:output_divide,:]\n",
        "    p_teacher = y_pred[output_divide: y_pred.shape[0], :]\n",
        "    student_loss = bce_loss(y_true, p_student)\n",
        "    total_teacher_loss = 0\n",
        "    # kl divergence in tensorflow requires all values to add up to 1, so done iteratively and averaged rather than applied to the entire output at once\n",
        "    for i in range(p_teacher.shape[0]):\n",
        "      for j in range(p_student.shape[1]):\n",
        "        total_teacher_loss += kl_loss([p_teacher[i][j], 1 - p_teacher[i][j]], [p_student[i][j], 1 - p_student[i][j]])\n",
        "    teacher_loss = total_teacher_loss / (p_teacher.shape[0] * p_student.shape[1])\n",
        "    final_loss = student_loss + teacher_loss\n",
        "    return  student_loss, teacher_loss, final_loss\n",
        "  return custom_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ4u9waJu99s"
      },
      "outputs": [],
      "source": [
        "# loss function to use if ignoring the effect of teacher network to evaluate base network only\n",
        "def custom_loss_wrapper(initial_value, lower_bound, total_steps, start_step):\n",
        "  bce_loss= tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "  def custom_loss(y_true, y_pred, step):\n",
        "    output_divide = int(y_pred.shape[0]/2)\n",
        "    p_student = y_pred[0:output_divide,:]\n",
        "    student_loss = bce_loss(y_true, p_student)\n",
        "    return  student_loss, student_loss, student_loss\n",
        "  return custom_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-uFIHZ8Su_6o"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "checkpoint = tf.train.Checkpoint(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lEa-luqvBFz"
      },
      "outputs": [],
      "source": [
        "#optimizer\n",
        "epochs = 2\n",
        "batch_size = 4\n",
        "steps_per_epoch = final_train_data[\"input_ids\"].shape[0] / batch_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "loss= custom_loss_wrapper()\n",
        "#metrics\n",
        "training_loop(\n",
        "    model,\n",
        "    final_train_data[\"input_ids\"],\n",
        "    final_train_data[\"attention_mask\"],\n",
        "    final_train_data[\"images\"],\n",
        "    final_train_labels,\n",
        "    final_eval_data[\"input_ids\"],\n",
        "    final_eval_data[\"attention_mask\"],\n",
        "    final_eval_data[\"images\"],\n",
        "    final_eval_labels,\n",
        "    epochs,\n",
        "    batch_size,\n",
        "    optimizer,\n",
        "    loss,\n",
        "    checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7po9rkmvEAQ"
      },
      "outputs": [],
      "source": [
        "# replace the number following ckpt with the checkpoint you would like to restore for final evaluation (choose checkpoint for which validation loss starts to increase)\n",
        "checkpoint.restore('/tmp/model/ckpt-1')\n",
        "\n",
        "teacher_preds_list = []\n",
        "student_preds_list = []\n",
        "for i in range(int(final_eval_labels.shape[0]/ batch_size)):\n",
        "  preds = model([final_eval_data[\"input_ids\"][i*batch_size:(i+1)*batch_size, :], final_eval_data[\"attention_mask\"][i*batch_size:(i+1)*batch_size, :], final_eval_data[\"images\"][i*batch_size:(i+1)*batch_size, :]])\n",
        "  output_divide = int(preds.shape[0]/2)\n",
        "  p_student = preds[0:output_divide, :]\n",
        "  p_teacher = preds[output_divide: preds.shape[0], :]\n",
        "  teacher_preds_list.append(p_teacher)\n",
        "  student_preds_list.append(p_student)\n",
        "\n",
        "teacher_preds = tf.concat(teacher_preds_list, 0)\n",
        "student_preds = tf.concat(student_preds_list, 0)\n",
        "total_preds = tf.concat([student_preds, teacher_preds], 0)\n",
        "\n",
        "print_metrics(final_eval_labels[0:teacher_preds.shape[0]], total_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFcytFouvId8"
      },
      "outputs": [],
      "source": [
        "def hierarchial_violations_helper(y_pred):\n",
        "  y_pred = tf.where(y_pred > threshold, 1, 0)\n",
        "  y_pred = tf.cast(y_pred, tf.int32)\n",
        "  violation_matrix = np.zeros((y_pred.shape[1], y_pred.shape[1]))\n",
        "  # row child, column parent\n",
        "  violation_count = 0\n",
        "  rows_with_violations = set()\n",
        "  for i in range(y_pred.shape[0]):\n",
        "    parent_node_indicies = set()\n",
        "    for j in range(y_pred.shape[1]):\n",
        "      if y_pred[i,j] == 1:\n",
        "        parent_nodes = parent_child_dict[id_to_node[j]]\n",
        "        for parent_node in parent_nodes:\n",
        "          parent_node_indicies.add(node_list_task_two[parent_node])\n",
        "          if y_pred[i, node_list_task_two[parent_node]] == 0:\n",
        "            violation_matrix[j, node_list_task_two[parent_node]] += 1\n",
        "    for parent_node_index in parent_node_indicies:\n",
        "      if y_pred[i, parent_node_index] == 0:\n",
        "        rows_with_violations.add(i)\n",
        "        violation_count += 1\n",
        "  return violation_count, violation_matrix\n",
        "violations, violation_matrix = hierarchial_violations_helper(student_preds)\n",
        "print(violations)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
